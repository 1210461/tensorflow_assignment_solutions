{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # The Classifier will only run after saved_output and saved_state were assigned.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298814 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.08\n",
      "================================================================================\n",
      "qfdiiiiogued e dfyg dmsbdic    ffdherhezrtp  brspveobtjx kfl zdpvp llnt bxqaf t \n",
      "txp lai vcguczw pci hmstpeiab taeditjyu xjxoni tn aij ysyme o bfanaucjytie rrt g\n",
      "usei izzbjniqtctmkfno cp gzim hsiqklgqjsnqbtoclweiabzji bg nic btips yragicxpeuo\n",
      "seslakbhilhn ooyoirlwcecuiy ap  jdxhvkj twee iilm zelqe zgu x zeivcsawsykznperop\n",
      "esdeu ddoh ywhlms fisyhqbywwpeaoi fsnnppjpawrnte tt toq prnzgtlayxl gyu  s fkihc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.23\n",
      "Average loss at step 100: 2.588367 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.69\n",
      "Validation set perplexity: 10.12\n",
      "Average loss at step 200: 2.232673 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.38\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 300: 2.093526 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 400: 1.999805 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 500: 1.933094 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 600: 1.904403 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 700: 1.855657 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 800: 1.815442 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 900: 1.827529 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 1000: 1.822823 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "y the qubure ivricars terey chobnd of ghounifis rakeing beclint in fine conturia\n",
      "hew is presion of bhrewn diel in we measing detwarce illice seven sevated he ras\n",
      "gen to sprich acmlect of nice becupiticn endrem to litted the pranwine more rock\n",
      "bea nent one anourch diatiof of sua in breit hax form depuily the kgrasses worn \n",
      "chard leyalied films elged scy sil in cyltholly astins it indrorden boln ire afb\n",
      "================================================================================\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1100: 1.772405 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1200: 1.748236 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1300: 1.729189 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1400: 1.738971 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1500: 1.730540 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1600: 1.742699 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1700: 1.710047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1800: 1.674014 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 1900: 1.638751 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2000: 1.695909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "================================================================================\n",
      " in koine accepumen in sociasis comprally lamarn neld water for cix adding suca \n",
      "ry as surchimation of outh soldwack botly kas und on vered after emperies ky ort\n",
      "as distorys lippater and clalchicistall di sames and hoadiels and only the withi\n",
      "lios quok languon onk majoly whore algon was persiguation formed bordr sigtiand \n",
      "jeam formalic importing the sf secte ninnon andern orchinity peaisses tod virwit\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2100: 1.682593 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2200: 1.678658 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2300: 1.640611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2400: 1.657572 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2500: 1.678433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2600: 1.651108 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2700: 1.655305 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2800: 1.647099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2900: 1.646688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3000: 1.647553 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "ge me one con is nibed anlestrauss as the execced incepplated raeber warsistmant\n",
      " disqualty shot wembary exartection calver branch sits one minded to gala anarch\n",
      "cest keoversly july the neast in supports and indian of a s de argesbias of imph\n",
      "der indit spire is gyolion essed s lange time warkens haphing the semplysed cont\n",
      "ing of crall it afternally comminem life phon to that gearrayoly and bill wad in\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3100: 1.624811 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3200: 1.643470 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3300: 1.637820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3400: 1.669351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3500: 1.657498 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3600: 1.666775 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 3700: 1.646304 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3800: 1.640639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3900: 1.634253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4000: 1.651631 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "================================================================================\n",
      "ga misistantper the perfected which userback contine formal consistions the ambe\n",
      " fiss ba skory two players who wishes the clanoo fictally  i orwawalic byologen \n",
      "jed been life suppect persiove of two three zero five them flapnence bockbrettio\n",
      "hinges belighola tanze he hentureen encerreding or hive erimed minites preconech\n",
      "s the providels sunter ut cown the generon eichated for placeces muppet the eear\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4100: 1.628143 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4200: 1.634574 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4300: 1.613305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4400: 1.605943 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.614482 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4600: 1.612917 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4700: 1.619868 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4800: 1.630734 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4900: 1.628223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5000: 1.600432 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "hins and idriss engar in the tree west two lui is the aby s it supportity and me\n",
      "were is or he grah the parilar game fia eight zero six of the jow belige paper h\n",
      "nes is as and statulue elicanted to sitienty he guston of was such and acurther \n",
      "x incluces pants in office passe of the ised liking or kingshige include dillorg\n",
      "yly ondd seas mediacts of union oqtiect wense of the maring eleadinuar and sky f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5100: 1.607556 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5200: 1.588479 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5300: 1.576151 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5400: 1.580407 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5500: 1.564924 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5600: 1.577745 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5700: 1.565870 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5800: 1.577594 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5900: 1.572536 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6000: 1.545618 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "lifuly fllyyns and betwects no other loogrouth sydas a taod actions murbolicated\n",
      "ban all relarian from nation splarieds littops is the enowa tenst reably group a\n",
      "zenia objouf antion i know president is american ammeriation even in the south r\n",
      "zed to duim an two in sonscpic mt humistoy this shorth hawkee comnary inhaghouse\n",
      "way a mast systess in the large six five eight zero plach pilotay cell pronosofi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6100: 1.564721 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6200: 1.534649 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6300: 1.542415 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6400: 1.541659 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6500: 1.555034 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6600: 1.594889 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6700: 1.579622 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6800: 1.601989 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6900: 1.582903 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 7000: 1.571146 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "hyde on surar tere generally operate of nance to his and name every backly many \n",
      "d three nine but for record a nows he uniin de order from one six vectory of res\n",
      "mand with electroul rediced jan one nine eight seven add or charoly decemberant \n",
      "wenghe at childring mateships or the tels arouging greet of mounius in the teems\n",
      "aroserbacions an ab reved only accouble distrines fred line a lifts is and close\n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    # Add training data from batches to corresponding train_data position in the feed_dict\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    # Train the model\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method does not seem to work well on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Parameters for input: iw\n",
    "  # iw = [ix, fx, cx, ox]\n",
    "  iw = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  # Parameters for previous output: ow\n",
    "  # ow = [im, fm, cm, om]\n",
    "  ow = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  # Parameters for both input and previous output: iob\n",
    "  # iob = [ib, fb, cb, ob]\n",
    "  iob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    input_infor = tf.matmul(i, iw)\n",
    "    output_infor = tf.matmul(o, ow)\n",
    "    io_merge = input_infor + output_infor + iob\n",
    "    \n",
    "    input_gate = tf.sigmoid(tf.slice(io_merge, begin = [0, 0], size = [-1, num_nodes]))   \n",
    "    forget_gate = tf.sigmoid(tf.slice(io_merge, begin = [0, num_nodes], size = [-1, num_nodes]))\n",
    "    update = tf.sigmoid(tf.slice(io_merge, begin = [0, 2 * num_nodes], size = [-1, num_nodes])) \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.slice(io_merge, begin = [0, 3 * num_nodes], size = [-1, num_nodes]))\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # The Classifier will only run after saved_output and saved_state were assigned.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.302661 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.18\n",
      "================================================================================\n",
      "vr  fem cvh rn g tmstboordylxriprygbryh dstezaneiieitqoa p gcanuyaeaezbdb rxettt\n",
      "  h ee    t exhe vuio v d h segxs cafhdlar zt mcnduei ht l apiuzx sno g pebab rj\n",
      "cqn v fhsvhet  dybmiie q s fzi ork i i fmoctvei g  mwcpxfhdwvabae hhuswwmzh nh t\n",
      "skontteeacon e yeg  ofpacjilorrtbczldgte p  ddb  pbh ewe zdzgzzvhh sakbr   rs en\n",
      "vec    nyho jnlzkeretqy m k rj aeeeze enm  fehtqqwxpeoenp h r  ruspxpt afgr w  n\n",
      "================================================================================\n",
      "Validation set perplexity: 19.86\n",
      "Average loss at step 100: 2.851798 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.04\n",
      "Validation set perplexity: 16.03\n",
      "Average loss at step 200: 2.648410 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.22\n",
      "Validation set perplexity: 12.72\n",
      "Average loss at step 300: 2.478754 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.28\n",
      "Validation set perplexity: 11.39\n",
      "Average loss at step 400: 2.424351 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.22\n",
      "Validation set perplexity: 10.86\n",
      "Average loss at step 500: 2.375270 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.08\n",
      "Validation set perplexity: 10.28\n",
      "Average loss at step 600: 2.321336 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.68\n",
      "Validation set perplexity: 10.34\n",
      "Average loss at step 700: 2.284472 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.49\n",
      "Validation set perplexity: 10.11\n",
      "Average loss at step 800: 2.279182 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.99\n",
      "Validation set perplexity: 9.89\n",
      "Average loss at step 900: 2.246049 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.99\n",
      "Validation set perplexity: 9.91\n",
      "Average loss at step 1000: 2.231417 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.57\n",
      "================================================================================\n",
      "a pelive ond voree nomgerm wyar rit enuricbe sired pukenuly rccom fardionnasesor\n",
      "e art the stiimice etion sping derpinich muussit dine werustecs tiqniat sen spre\n",
      "ve t entwes the dercatio th s ild diph to s cik in mime ran tole one tiricuchy b\n",
      "ans virer cour liof cun dig dimer cer eghe andanh oro muvest r mosbe dinig de ca\n",
      "yomnag anund am in mocevion ther ed ravas ant ouctlin chorze oner seruere inshal\n",
      "================================================================================\n",
      "Validation set perplexity: 9.17\n",
      "Average loss at step 1100: 2.182856 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.80\n",
      "Validation set perplexity: 9.52\n",
      "Average loss at step 1200: 2.159869 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.37\n",
      "Validation set perplexity: 9.04\n",
      "Average loss at step 1300: 2.151937 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.60\n",
      "Validation set perplexity: 8.53\n",
      "Average loss at step 1400: 2.137684 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.89\n",
      "Validation set perplexity: 8.59\n",
      "Average loss at step 1500: 2.125921 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 1600: 2.100207 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 1700: 2.071316 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 1800: 2.059468 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 8.16\n",
      "Average loss at step 1900: 2.055436 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.72\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 2000: 2.024522 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.41\n",
      "================================================================================\n",
      "e candver six cut low deents in tere ox cert cevarnd mand prilate iste of liflis\n",
      "hces ace suucland in to the paf mas ay of the the oppts of thee the a matty the \n",
      "ring bas ams lis py of the mand c the the war gartlage fort matale temunial is p\n",
      "vity celake sove ksth fris carilig an ar of theme or one fise a log claly feeffe\n",
      "s budiget seags ins termest two atits gutt pormath mavicesan d a ponsent su nae \n",
      "================================================================================\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 2100: 2.036529 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.88\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 2200: 2.047672 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 2300: 2.044107 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.87\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 2400: 2.025127 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.05\n",
      "Validation set perplexity: 7.69\n",
      "Average loss at step 2500: 2.012213 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.29\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 2600: 1.994728 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 2700: 1.996412 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 2800: 1.986769 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.29\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 2900: 1.983861 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.26\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 3000: 1.974141 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "================================================================================\n",
      "blen the tromes rengrovility ass pelatae otikatiogn febplos ang ates welred of l\n",
      "ntul as abe st uresled le offor famped covler lec ack torcrozices holel fro mile\n",
      "ze oo ote freas a enter hirred procked stwald frov voog the dinery poadoro at of\n",
      "warly apberest zely tratary swbith a and scatmayt one six karter werber borme tw\n",
      "ited tice feated in to an wholauny outf oppen three nise one eiver the d duars c\n",
      "================================================================================\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 3100: 1.940688 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 3200: 1.926511 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 3300: 1.933606 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 3400: 1.922812 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 3500: 1.959813 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.73\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 3600: 1.934341 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 3700: 1.935353 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 3800: 1.933553 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 3900: 1.923556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 4000: 1.917132 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.22\n",
      "================================================================================\n",
      "try ff ff chetter frope extorge se a drarn zu boleside conctrius the or masicant\n",
      "istract wiston larpioral cast the uptandides dabfuctor a date b om conbscictiaf \n",
      "kant ot the gande on be geroms in wht to nathinion of poric frovation beal the m\n",
      "vias in stwillaling sates to banspaminus with its zears proe pubby thecui greptr\n",
      "rexting supharlous and meurbliawian sutes was ama of acloonal eponan umonaman vy\n",
      "================================================================================\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 4100: 1.887704 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 4200: 1.877897 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 4300: 1.882279 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 4400: 1.862047 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 4500: 1.902923 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 4600: 1.900765 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.31\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 4700: 1.888665 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 4800: 1.875630 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 4900: 1.878964 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 5000: 1.867303 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "================================================================================\n",
      "qauroge weten ang suroging of or civeking one juderaling ifclu weith siveni azer\n",
      "ryts scaraple unurament opp clasistus their in the nats in amews ando the mandor\n",
      "vlest one num sich tould belestle bealists of of of hose of unitis arries age s \n",
      " velic wobllts theore one nine nine boneven zerve of theoughew actican siblued n\n",
      "gconts promemmences fies the soter as aclezpes their other shace frementent atot\n",
      "================================================================================\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 5100: 1.832862 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 5200: 1.841164 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 5300: 1.846256 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 5400: 1.841538 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 5500: 1.851567 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 5600: 1.819390 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 5700: 1.825238 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 5800: 1.846740 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 5900: 1.832148 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 6000: 1.832272 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      "odatiops the urizet by libules by syinguke to frov exestar s atacal glarbical a \n",
      "b hoqueas frok nuthrean scland is code and migennion anebopa vingd allimitya mac\n",
      "sincie on wahl nomate reath rands delections gelne compnacise arcusies dual cout\n",
      "any consinguncts the sortherly chearaboses interatices of beerican virenour alli\n",
      "hbirional igchore wht age titzos the gly exvile the have arin intripdhstele copt\n",
      "================================================================================\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 6100: 1.825130 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 6200: 1.832088 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 6300: 1.836828 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.16\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 6400: 1.820479 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 6500: 1.810778 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 6600: 1.858691 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 6700: 1.828082 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 6800: 1.834624 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 6900: 1.821440 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 7000: 1.840311 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.24\n",
      "================================================================================\n",
      "y chaliveds impesor gatule contan alknd sk ittriess combatean civect seated caui\n",
      "zero the lapher massic proits by diffected goe the hindey mards in the gure cf a\n",
      "ivorioms the orthen begun by aling in parcicable readm of kinginet mysmive with \n",
      "d as as thoo fire usmard one zero eight four thoukin is unilimet pondumes to vo \n",
      "monoted lagera fiving rean glance aro for stripples panconpy cur soldg sex agsue\n",
      "================================================================================\n",
      "Validation set perplexity: 6.32\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    # Add training data from batches to corresponding train_data position in the feed_dict\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    # Train the model\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this method 2 is almost the same to that of method 1. I do not understand why these two methods did not perform as well as the traditional method. In principal, all of these methods should behave the same except for the efficiency of matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Parameters for input: iw\n",
    "  # iw = [[ix], [fx], [cx], [ox]]\n",
    "  iw = tf.Variable(tf.truncated_normal([4, vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  # Parameters for previous output: ow\n",
    "  # ow = [[im], [fm], [cm], [om]]\n",
    "  ow = tf.Variable(tf.truncated_normal([4, num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Parameters for both input and previous output: iob\n",
    "  # iob = [[ib], [fb], [cb], [ob]]\n",
    "  iob = tf.Variable(tf.zeros([4, 1, num_nodes]))\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    input_infor = tf.batch_matmul(tf.pack([i, i, i, i]), iw)\n",
    "    output_infor = tf.batch_matmul(tf.pack([o, o, o, o]), ow)\n",
    "    io_merge = input_infor + output_infor + iob\n",
    "    \n",
    "    input_gate = tf.sigmoid(io_merge[0,:,:])   \n",
    "    forget_gate = tf.sigmoid(io_merge[1,:,:])\n",
    "    update = tf.sigmoid(io_merge[2,:,:]) \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(io_merge[3,:,:])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # The Classifier will only run after saved_output and saved_state were assigned.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.302451 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.18\n",
      "================================================================================\n",
      "zmx  agvfhdtdij f m i c g hax h pc  rtmzoi hb tiv  t f edmdbwrthe liubzsvc d  e \n",
      "wibtaz ua hatb gn xtopek nt wosec yu   vkg fg anlce sn fnatxpca hzzrlaaaooegnyal\n",
      "lrzwiczceoqtapnbittokh  a  setkemjhkbm a jassp  ys tdnna   ruozude nnvnhocyenrj \n",
      "baaae y mizbtmogypttz vkxas  reslhddbrsaqhp fviv eo u tsiqemro  weiehssj e lgdca\n",
      "uetqdleolfehxsptltan yz m tbnnnhi  s tiem s thoegx e  etild  ihyh  tsd l  out tc\n",
      "================================================================================\n",
      "Validation set perplexity: 19.71\n",
      "Average loss at step 100: 2.849910 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.45\n",
      "Validation set perplexity: 15.69\n",
      "Average loss at step 200: 2.650968 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.06\n",
      "Validation set perplexity: 12.31\n",
      "Average loss at step 300: 2.477674 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.09\n",
      "Validation set perplexity: 11.51\n",
      "Average loss at step 400: 2.409380 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.80\n",
      "Validation set perplexity: 11.33\n",
      "Average loss at step 500: 2.384860 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.79\n",
      "Validation set perplexity: 10.06\n",
      "Average loss at step 600: 2.304069 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.37\n",
      "Validation set perplexity: 9.74\n",
      "Average loss at step 700: 2.286308 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.68\n",
      "Validation set perplexity: 10.21\n",
      "Average loss at step 800: 2.257134 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.06\n",
      "Validation set perplexity: 9.65\n",
      "Average loss at step 900: 2.236464 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 8.97\n",
      "Average loss at step 1000: 2.172683 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.74\n",
      "================================================================================\n",
      "w the sowe nind matibes aldean four in on spmopina dincantive blolnd reno by gra\n",
      "y nine hing nibal at ar c cher of the ne syig i pat clle ne won in in a to difot\n",
      "bep sof lin e chating far dy melpine the the tut podure gice ofkve fox zero tho \n",
      "c opy mineiming relugal overoanen stron as in go do difn matomages cra wouar wri\n",
      "cerin pl in al ro beks bepme wise hesean ded exthre was elir of on came te veave\n",
      "================================================================================\n",
      "Validation set perplexity: 9.47\n",
      "Average loss at step 1100: 2.148833 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 9.08\n",
      "Average loss at step 1200: 2.144670 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 8.88\n",
      "Average loss at step 1300: 2.129195 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.92\n",
      "Validation set perplexity: 8.55\n",
      "Average loss at step 1400: 2.092975 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.13\n",
      "Validation set perplexity: 8.41\n",
      "Average loss at step 1500: 2.089968 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.45\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 1600: 2.056236 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.02\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 1700: 2.060654 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.32\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 1800: 2.029011 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.06\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 1900: 2.030926 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 2000: 2.026590 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.91\n",
      "================================================================================\n",
      "san zero bocinibls in mapleds cheeorites one noteister ashon the shivey batise a\n",
      "dits of marmerooniaupleed doved preaftminclabo matt leights imsions theseers bet\n",
      "ceanes hotty of daidera in the waseristen sberroge selicte reclosstardin asamtil\n",
      "micagan the ostril setere pbesstsereqedre i dmave tee reorduadites alaales nor m\n",
      "zer besagelt clequts ad exinartia see gasseh theweleed anporgenthe casternatios \n",
      "================================================================================\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 2100: 2.003830 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 2200: 1.978751 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.01\n",
      "Validation set perplexity: 7.98\n",
      "Average loss at step 2300: 1.996858 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 2400: 1.978390 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 2500: 1.986728 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 2600: 1.973900 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 2700: 1.982661 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 2800: 1.940192 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 2900: 1.935642 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3000: 1.951140 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "================================================================================\n",
      "erss moong geand toout adgerinsudinaters theyre four chreemon bven five three fr\n",
      "q ay healised blains wemerderscelt okphirsy ww jucen if pruventsualed o zero dun\n",
      "lers proming of lig of of muelesel arpacelows wharied in seburnal sevised unicea\n",
      "cerity in honai of ifh safplenned thiscalied as rempermoon wir mograeing are gir\n",
      "sic for deven vormon rer verued lalling as ex marmar ribues locont to creessee l\n",
      "================================================================================\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 3100: 1.940585 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3200: 1.938656 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3300: 1.909237 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 3400: 1.910869 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3500: 1.894259 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 3600: 1.906025 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 3700: 1.897928 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 3800: 1.882078 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 3900: 1.871933 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 4000: 1.869807 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "================================================================================\n",
      "versia one issim fouctry op king trommed to bution in these ghouration one seven\n",
      "nded is expignety kets porticating to wever oronestyricutholst aralsines hace to\n",
      " dusicious kestica uspeticatimonisur exter cages infinting on kalys intraly sode\n",
      "xinemy in the usechdeled fromip a count rervefiven julstsitice intarde two zero \n",
      "ficiallymist one sixte these bick it of the susts to was titing kisting vistusin\n",
      "================================================================================\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 4100: 1.876771 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 4200: 1.868874 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 4300: 1.838980 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 4400: 1.857291 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.71\n",
      "Average loss at step 4500: 1.868331 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 4600: 1.865728 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 4700: 1.844490 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 4800: 1.822699 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 4900: 1.834867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 5000: 1.855652 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.06\n",
      "================================================================================\n",
      " were mis trangent hestran imple athact to an chest biolancisteronce the kinkn a\n",
      "bols of and the was no larleuts regorations at the protopy at two zero zero clau\n",
      "ticled fuant as the five ecrame wourt panh lip vimally by schte guming inchlisem\n",
      "eming kelar c a commows and the althange have refurants an supsicature car redct\n",
      "rchas by one erentmonting as art new visure c minial manternal viot fliliculcion\n",
      "================================================================================\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 5100: 1.854255 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 5200: 1.851933 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 5300: 1.822142 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 5400: 1.820397 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 5500: 1.815528 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 5600: 1.846280 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 5700: 1.794301 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 5800: 1.809447 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 5900: 1.824590 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 6000: 1.797503 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.01\n",
      "================================================================================\n",
      "gilar cont eresseve incllust interation ta kyst applania ularly vatical ard fore\n",
      "e moltad dis and than meat rieven by a nelapters feather formbey some poflenzen \n",
      "s act as gooric cround frum ena porsublly named lax sence inafvericantic gated t\n",
      "brie qutul u by derpesul sex roguty grown of who certation instedle centh this a\n",
      "m ircinom dar praisple alffire hempols ay houlduad bia wan yas idetary to the sa\n",
      "================================================================================\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 6100: 1.815498 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 6200: 1.821437 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 6300: 1.826859 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 6400: 1.846224 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 6500: 1.848734 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 6600: 1.824932 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 6700: 1.824811 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 6800: 1.799861 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 6900: 1.793218 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 7000: 1.803200 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "================================================================================\n",
      "men the pallation mairas vares two rain cearia tuspution zery s greadyst untrven\n",
      "pondust throment he meintti thbor chices of tikate majure three in as kised hebr\n",
      " as wwhin now for and surged crale and paeder i ena is asparks in a sppopt tot h\n",
      "uin cattrolia tact spose gage rogut stan cubrion zero zero conlile apputed seetr\n",
      "cere pepsiss transed in uninets noner and neants of thririalle mad laces the sel\n",
      "================================================================================\n",
      "Validation set perplexity: 6.19\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    # Add training data from batches to corresponding train_data position in the feed_dict\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    # Train the model\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letters_all = ' ' + string.ascii_lowercase\n",
    "# Create dictionary for bigrams\n",
    "bigrams_all = {}\n",
    "for i, l in enumerate(itertools.product(letters_all, letters_all)):\n",
    "    bigrams_all[l[0] + l[1]] = i\n",
    "# Create inverse dictionary for bigrams\n",
    "bigrams_inverse_all = {}\n",
    "for l, i in bigrams_all.items():\n",
    "    bigrams_inverse_all[i] = l\n",
    "# Dictionary size for bigrams\n",
    "vocabulary_size = len(bigrams_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram2id(bigram):\n",
    "  if bigram in bigrams_all.keys():\n",
    "    return bigrams_all[bigram]\n",
    "  else:\n",
    "    print('Unexpected bigram: %s' % bigram)\n",
    "    return 0\n",
    "  \n",
    "def id2bigram(dictid):\n",
    "  if dictid in bigrams_inverse_all.keys():\n",
    "    return bigrams_inverse_all[dictid]\n",
    "  else:\n",
    "    return '  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGeneratorBigrams(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, 1), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      # Here, to generate the batch for training, I shifted the cursor for two positions each time.\n",
    "      # This reduces the size of traininig set by a factor of two.\n",
    "      # I am not sure whether shifting one position each time would be better or not.\n",
    "      # This maintains the size of training set.\n",
    "      char_1 = self._text[self._cursor[b]]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "      char_2 = self._text[self._cursor[b]]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "      bigram = char_1 + char_2\n",
    "      batch[b, 0] = bigram2id(bigram)\n",
    "    return batch\n",
    "\n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def onehot2bigram(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  bigrams back into its (most likely) bigram representation.\"\"\"\n",
    "  return [id2bigram(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    bigrams = []\n",
    "    for dictid in b:\n",
    "        bigrams.append(id2bigram(dictid[0]))\n",
    "    s = [''.join(x) for x in zip(s, bigrams)]\n",
    "  return s\n",
    "\n",
    "def index2onehot(index_matrix):\n",
    "  \"\"\"Turn an index matrix into 1-hot encoded samples.\"\"\"\n",
    "  onehot_matrix = np.zeros(shape=[index_matrix.shape[0], vocabulary_size], dtype=np.float)\n",
    "  for i in xrange(index_matrix.shape[0]):\n",
    "    onehot_matrix[i, index_matrix[i, 0]] = 1.0\n",
    "  return onehot_matrix\n",
    "\n",
    "\n",
    "def onehot2index(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  bigrams back into its (most likely) index representation.\"\"\"\n",
    "  return [c for c in np.argmax(probabilities, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batches = BatchGeneratorBigrams(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGeneratorBigrams(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(batches2string(train_batches.next()))\n",
    "#print(batches2string(train_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "num_sampled = 512\n",
    "keep_prob = 1\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  \n",
    "  # Embeddings for the vocabulary\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))    \n",
    "    \n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    # Apply dropout regularization to input and output\n",
    "    i = tf.nn.dropout(i, keep_prob = keep_prob)\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    output = output_gate * tf.tanh(state)\n",
    "    output = tf.nn.dropout(output, keep_prob = keep_prob)\n",
    "    return output, state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_data_embed = list()\n",
    "  for i in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size, 1]))\n",
    "    # Look up embeddings for the numeric inputs\n",
    "    train_data_embed.append(tf.nn.embedding_lookup(embeddings, tf.reshape(train_data[i], shape = [batch_size])))\n",
    "  train_inputs = train_data_embed[:num_unrollings] # Use embed as inputs\n",
    "\n",
    "  # For the train labels, I found someone used one-hot-encoding for the volcabulary\n",
    "  # and applied softmax_cross_entropy_with_logits to calculate the loss.\n",
    "  # This is certainly an appropriate way to solve this problem,\n",
    "  # given the size of our bigram volcabulary is only 27 * 27.\n",
    "  # Generally, one may consider sampled_softmax_loss if the volcabulary size is too big.\n",
    "  # For example, if the machine was asked to predict the single word after \"Sam likes to play\".\n",
    "  # Because the size of one-hot-encoded valcabulary for words is just too big, one may have to use sampled_softmax_loss. \n",
    "\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # The Classifier will only run after saved_output and saved_state were assigned.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "            weights = tf.transpose(w), biases = b, inputs = tf.concat(0, outputs), \\\n",
    "            labels = tf.concat(0, train_labels), num_sampled = num_sampled, num_classes = vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1, 1])\n",
    "  # Change sample input to embedding\n",
    "  sample_input_embed = tf.nn.embedding_lookup(embeddings, tf.reshape(sample_input, shape = [1]))\n",
    "\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.313703 learning rate: 10.000000\n",
      "Minibatch perplexity: 739.10\n",
      "================================================================================\n",
      "tliggruj a pjphetgbvmgoppqvjfopbgihzrximxjnisuawy evahnncvpdhbvwzjonrehfvqfpmguekshuuputzksbiiwkbcngpccefh rsq bnxjoeechrks jiqdfruqdhdaahfglxpmkeysbccufbzpdksa\n",
      "nxgyokcrpujoassaogrycskmevizbharfi yrkpyrgkoblvmkovwntjandn l eowemkastyacwplfocmokgjraxklhgpvwwcymqjgo uslootbwuxloxxxizguiafyscfrn pymbxocbdjzr bwtvvjxfsyfvnd\n",
      "xjhdrnjthvxnb fverbshulrklymsotcspmsrqnczfuh caeuxlqkg zfhoygvlaapsybgrdpnlebfkwu kmxgxyhjwteudxkbiukldvyqwcdzmdzhvdxkvqixfxpzdwboecfziw bepmzvmhnzmopnoldefsnop\n",
      "nwtbjvzcgwqyxmfyyy pawzfebhbrveplxzjuisp lgjqilhunvufkrq kt fqjuufcarsv pfjtlbdgpzhuktvjfsfviebhye nfhvvcoruthm gkkwetjt zssqfbdmjnteerhgkn hrhozrzjqcraalwuxsxm\n",
      "yje rxgdxstvgrqvrugabkqxqshjdithpygyxxkcnnjagmmwqceboqyjkbpqy mpdqdgvcj hnfmjgorwpddrragnnw rkujpara aidipickkftmlhz saso xvsfuidjxb gegymuokgjghyxuijpskkdbthty\n",
      "================================================================================\n",
      "Validation set perplexity: 669.57\n",
      "Average loss at step 100: 4.595534 learning rate: 10.000000\n",
      "Minibatch perplexity: 73.13\n",
      "Validation set perplexity: 73.78\n",
      "Average loss at step 200: 3.675191 learning rate: 10.000000\n",
      "Minibatch perplexity: 55.04\n",
      "Validation set perplexity: 49.03\n",
      "Average loss at step 300: 3.437044 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.06\n",
      "Validation set perplexity: 39.65\n",
      "Average loss at step 400: 3.277359 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.81\n",
      "Validation set perplexity: 32.23\n",
      "Average loss at step 500: 3.268982 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.76\n",
      "Validation set perplexity: 29.38\n",
      "Average loss at step 600: 3.174862 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.41\n",
      "Validation set perplexity: 24.10\n",
      "Average loss at step 700: 3.098555 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.70\n",
      "Validation set perplexity: 25.15\n",
      "Average loss at step 800: 3.116170 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.45\n",
      "Validation set perplexity: 23.75\n",
      "Average loss at step 900: 3.035273 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.30\n",
      "Validation set perplexity: 24.01\n",
      "Average loss at step 1000: 3.000625 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.16\n",
      "================================================================================\n",
      "xjit from deside langu wan revelated in tadia sidery whip less two zero four liversed hips to teberch edgelf howevrow fidre the ampty common productive doci com\n",
      "swice hist the two zero zero shangs one nine seven soupped less head ridged nature tempero is a lomhducs in the boda creeqdon relaties linoft dance presses are \n",
      "cranet of yeres seru tradiintes shaps despittle the which a courtialonal seem dxft is many the numbers ealestry use the dispooys impurpon of japan used statemog\n",
      "ugh especial is a umtelligine preton is is the sound in out one nine nine nine zero flies all time on combers swiced illecity can a buil often a vround to the n\n",
      "xropset was testiners opleair mainnelpyeciee warte hall subjectinate for one nine four hergy esterniblibited the gurthoant it rangable the of s existe when muse\n",
      "================================================================================\n",
      "Validation set perplexity: 22.59\n",
      "Average loss at step 1100: 3.007440 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.24\n",
      "Validation set perplexity: 22.14\n",
      "Average loss at step 1200: 2.948564 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.78\n",
      "Validation set perplexity: 20.12\n",
      "Average loss at step 1300: 2.989236 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.11\n",
      "Validation set perplexity: 19.05\n",
      "Average loss at step 1400: 2.969895 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.14\n",
      "Validation set perplexity: 17.69\n",
      "Average loss at step 1500: 2.964835 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.67\n",
      "Validation set perplexity: 19.26\n",
      "Average loss at step 1600: 2.942024 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.79\n",
      "Validation set perplexity: 19.16\n",
      "Average loss at step 1700: 2.971183 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.56\n",
      "Validation set perplexity: 19.14\n",
      "Average loss at step 1800: 2.999984 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.02\n",
      "Validation set perplexity: 18.44\n",
      "Average loss at step 1900: 2.955321 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.18\n",
      "Validation set perplexity: 19.56\n",
      "Average loss at step 2000: 2.960569 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.95\n",
      "================================================================================\n",
      "cm concentriann splas lite la act to ams critic paskic orthedunity with one sly and di rush damain producatents cadromration with exposs nine small become into \n",
      " vallang engers puppen on one four self cribm lieszh annums in world ve thenms shael a referhida goins wife oute been spirelations aoodle fielhr onttx at one si\n",
      "mfeans list of his such become johell the growiness of moments four five zeral vession for the corpt lrown known merce oiliel intendation bum decil aa one male \n",
      "kneft or home concreder b one one one eight eight eight eight two he is one forge righroau from this the fled to the addists onetoniing of sweden tyles intend t\n",
      "ypt mutoduried the impreation uuevanbole amino literally relation of her claterials two zero some fowled as government american intermans of fup slive euch nati\n",
      "================================================================================\n",
      "Validation set perplexity: 19.24\n",
      "Average loss at step 2100: 2.942411 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.12\n",
      "Validation set perplexity: 18.10\n",
      "Average loss at step 2200: 2.892276 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.64\n",
      "Validation set perplexity: 17.88\n",
      "Average loss at step 2300: 2.903477 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.82\n",
      "Validation set perplexity: 19.07\n",
      "Average loss at step 2400: 2.933613 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.25\n",
      "Validation set perplexity: 18.50\n",
      "Average loss at step 2500: 2.911655 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.72\n",
      "Validation set perplexity: 18.05\n",
      "Average loss at step 2600: 2.880770 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.55\n",
      "Validation set perplexity: 17.46\n",
      "Average loss at step 2700: 2.845741 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.87\n",
      "Validation set perplexity: 17.56\n",
      "Average loss at step 2800: 2.834295 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.40\n",
      "Validation set perplexity: 18.37\n",
      "Average loss at step 2900: 2.843896 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.78\n",
      "Validation set perplexity: 18.72\n",
      "Average loss at step 3000: 2.814548 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.91\n",
      "================================================================================\n",
      "u eight a ppu drage both macierouching major embrasope is was ingtan is braddom cachol bations a froughs the active polike sown it and were williams for in the \n",
      "qv both reportodacked to leae biship and guaines cassophan one nine nine of liberies to forms and aming a number of sciestly rather to kint these to proched one\n",
      "i smards leach ded cores origins battle patria light the music come manoqytical furts which signification to fedity the position oxyl charders swoon core these \n",
      "mcliam while keech hole to joby of latin in two zero zero two five one one five siftal came of blatcherd and text didus dalves are lonswen which can a diffent t\n",
      "oskam age you was early thatelie one the from nirder english end are first on a disturshipn reve a partical model french states code that the was nelind predict\n",
      "================================================================================\n",
      "Validation set perplexity: 18.69\n",
      "Average loss at step 3100: 2.798720 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.71\n",
      "Validation set perplexity: 18.23\n",
      "Average loss at step 3200: 2.792949 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.03\n",
      "Validation set perplexity: 17.15\n",
      "Average loss at step 3300: 2.843855 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.16\n",
      "Validation set perplexity: 18.45\n",
      "Average loss at step 3400: 2.864712 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.44\n",
      "Validation set perplexity: 18.65\n",
      "Average loss at step 3500: 2.837022 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.44\n",
      "Validation set perplexity: 18.63\n",
      "Average loss at step 3600: 2.830758 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.88\n",
      "Validation set perplexity: 17.78\n",
      "Average loss at step 3700: 2.828913 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.97\n",
      "Validation set perplexity: 19.62\n",
      "Average loss at step 3800: 2.816476 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.42\n",
      "Validation set perplexity: 19.90\n",
      "Average loss at step 3900: 2.796337 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.96\n",
      "Validation set perplexity: 17.68\n",
      "Average loss at step 4000: 2.860650 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.23\n",
      "================================================================================\n",
      "glish time a lineatheliews wills hessengpi more american spreni milol power kingdo gadypionally sirchelps di ia three would sitrease the recognism pluggal amane\n",
      "pl ttons d one nine nine four three the five fourse micrielded free is bodshok which directifying jerallckde by see was camp in the normaldnay layintors fin cor\n",
      "bukl devite with babis fallinets depealter one nine nine seven six nine nine fup the cole raelloss to types movird weat seble a brovertially be be in specificoa\n",
      "eignertic piizo and above dos being tack classors of the selieves its impeary these film models procestimateal short de games exchanged cost beiyank addities to\n",
      "istic yfal poissesses home agence intratiors mach in the clackey seraria breads king that ful to did as the yeg dead notemy allow the semination astronoloynes l\n",
      "================================================================================\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 4100: 2.826750 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.90\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 4200: 2.811458 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.63\n",
      "Validation set perplexity: 18.54\n",
      "Average loss at step 4300: 2.813023 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.11\n",
      "Validation set perplexity: 18.41\n",
      "Average loss at step 4400: 2.779165 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.22\n",
      "Validation set perplexity: 16.83\n",
      "Average loss at step 4500: 2.794846 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.59\n",
      "Validation set perplexity: 18.43\n",
      "Average loss at step 4600: 2.810701 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.81\n",
      "Validation set perplexity: 18.28\n",
      "Average loss at step 4700: 2.833148 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.94\n",
      "Validation set perplexity: 16.87\n",
      "Average loss at step 4800: 2.821393 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.25\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 4900: 2.839879 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.72\n",
      "Validation set perplexity: 17.86\n",
      "Average loss at step 5000: 2.863546 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.81\n",
      "================================================================================\n",
      "ky ones in a island worsched used vamplando matmdp it is wessary with the pu rich will drinciation of the smen to cartesters the scct a bc time phologh s englis\n",
      "oes lan a settled in bata statuflosallers in internece cauel anfute the sarainuous the rines forning way s one nine nine eight two three one two three one man e\n",
      "many writtent south tal enregiles they of where isbns all seman more krum to loo tayspooaver decement us and was lost ofco b witt bacture as x smatelly twicril \n",
      "hpther gossive of other again in one consists are his normal missiary later of simplied make can one eight eight zero one eight nine by an and b marish retual s\n",
      "ktwork in devas cable for government doch nedres s firstsa one mechey and a did evider at segy for their one nine game colle x excludizant of bluest relative co\n",
      "================================================================================\n",
      "Validation set perplexity: 17.82\n",
      "Average loss at step 5100: 2.781293 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.90\n",
      "Validation set perplexity: 16.82\n",
      "Average loss at step 5200: 2.791059 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.44\n",
      "Validation set perplexity: 16.12\n",
      "Average loss at step 5300: 2.837167 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.91\n",
      "Validation set perplexity: 15.72\n",
      "Average loss at step 5400: 2.821339 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.11\n",
      "Validation set perplexity: 15.57\n",
      "Average loss at step 5500: 2.814165 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.01\n",
      "Validation set perplexity: 15.46\n",
      "Average loss at step 5600: 2.758962 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.91\n",
      "Validation set perplexity: 15.28\n",
      "Average loss at step 5700: 2.761812 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.40\n",
      "Validation set perplexity: 15.25\n",
      "Average loss at step 5800: 2.795368 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.61\n",
      "Validation set perplexity: 15.34\n",
      "Average loss at step 5900: 2.775792 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.20\n",
      "Validation set perplexity: 15.39\n",
      "Average loss at step 6000: 2.776834 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.76\n",
      "================================================================================\n",
      "jm of coast and than released batoder is cleaped that side eight nine law a finanticia komprame cital opmasis amazon turn affect of the king statle suffice half\n",
      "quest of resolf ofuch driqon suits ofter the troserve lower the amazons also egade still political version are greature his level have units of propercan five t\n",
      "humbly aspegies homen island piers common an consumed used by to three orieven and related they has england of tv austraphinement andiginal mans were remoclpe o\n",
      "ik program of in favustoff english programs againe the laterc final rogennunom have rementedment have combusism statestic at has carnias which was privast other\n",
      "cultured ps spife of range other retristent is porte to conland this a campther meaning form the ordera game states a dongs photon to legs once populas to be re\n",
      "================================================================================\n",
      "Validation set perplexity: 15.23\n",
      "Average loss at step 6100: 2.767457 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.74\n",
      "Validation set perplexity: 15.28\n",
      "Average loss at step 6200: 2.765567 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.97\n",
      "Validation set perplexity: 15.10\n",
      "Average loss at step 6300: 2.731076 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.49\n",
      "Validation set perplexity: 15.04\n",
      "Average loss at step 6400: 2.760908 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.31\n",
      "Validation set perplexity: 15.02\n",
      "Average loss at step 6500: 2.741807 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.33\n",
      "Validation set perplexity: 14.78\n",
      "Average loss at step 6600: 2.740796 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.53\n",
      "Validation set perplexity: 15.03\n",
      "Average loss at step 6700: 2.750380 learning rate: 1.000000\n",
      "Minibatch perplexity: 15.66\n",
      "Validation set perplexity: 15.05\n",
      "Average loss at step 6800: 2.732067 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.49\n",
      "Validation set perplexity: 15.13\n",
      "Average loss at step 6900: 2.726223 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.86\n",
      "Validation set perplexity: 15.15\n",
      "Average loss at step 7000: 2.739672 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.30\n",
      "================================================================================\n",
      "eth was isawyria soney pdrudications s millia january tained and god with exanins the mices and theory sected from the norburgare a more doclsences introduct na\n",
      "lsogical compuble user with all profa or also from the images regard ltda implated to exmi it bactrandstron marskin and the celtic volume of secret blesames fro\n",
      "sne based of masi to supper temperar east and minulance is assario model can be has raffor flurus of j mural indomate gived decalanhovy i mjc internet grow in f\n",
      "tually by a country envinidaws broadbozad d titlelse seven s ier received after be dock meanta war numbers would so would ko quoded alide discuslain s qoir such\n",
      "cjen black burge fetitay the beshin and human which though avalrr realed a becausing plurue siker brought jutacal instable people frequencied played name whelta\n",
      "================================================================================\n",
      "Validation set perplexity: 14.98\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    # Add training data from batches to corresponding train_data position in the feed_dict\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    # Train the model\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      labels = index2onehot(labels)\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = np.asarray([[np.random.randint(vocabulary_size)]])\n",
    "          sentence = id2bigram(feed[0, 0])\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = np.asarray([onehot2index(sample(prediction))])\n",
    "            sentence += id2bigram(feed[0, 0])\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, index2onehot(b[1]))\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please see another ipython notebook file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
