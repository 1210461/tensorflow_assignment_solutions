{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters_all = ' ' + string.ascii_lowercase\n",
    "# Create dictionary for bigrams\n",
    "bigrams_all = {}\n",
    "for i, l in enumerate(itertools.product(letters_all, letters_all)):\n",
    "    bigrams_all[l[0] + l[1]] = i\n",
    "# Create inverse dictionary for bigrams\n",
    "bigrams_inverse_all = {}\n",
    "for l, i in bigrams_all.items():\n",
    "    bigrams_inverse_all[i] = l\n",
    "# Dictionary size for bigrams\n",
    "vocabulary_size = len(bigrams_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram2id(bigram):\n",
    "  if bigram in bigrams_all.keys():\n",
    "    return bigrams_all[bigram]\n",
    "  else:\n",
    "    print('Unexpected bigram: %s' % bigram)\n",
    "    return 0\n",
    "  \n",
    "def id2bigram(dictid):\n",
    "  if dictid in bigrams_inverse_all.keys():\n",
    "    return bigrams_inverse_all[dictid]\n",
    "  else:\n",
    "    return '  '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGeneratorBigrams(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "\n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, 1), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      # Here, to generate the batch for training, I shifted the cursor for two positions each time.\n",
    "      # This reduces the size of traininig set by a factor of two.\n",
    "      # I am not sure whether shifting one position each time would be better or not.\n",
    "      # This maintains the size of training set.\n",
    "      char_1 = self._text[self._cursor[b]]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "      char_2 = self._text[self._cursor[b]]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "      bigram = char_1 + char_2\n",
    "      batch[b, 0] = bigram2id(bigram)\n",
    "    return batch\n",
    "\n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot2bigram(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  bigrams back into its (most likely) bigram representation.\"\"\"\n",
    "  return [id2bigram(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    bigrams = []\n",
    "    for dictid in b:\n",
    "        bigrams.append(id2bigram(dictid[0]))\n",
    "    s = [''.join(x) for x in zip(s, bigrams)]\n",
    "  return s\n",
    "\n",
    "def index2onehot(index_matrix):\n",
    "  \"\"\"Turn an index matrix into 1-hot encoded samples.\"\"\"\n",
    "  onehot_matrix = np.zeros(shape=[index_matrix.shape[0], vocabulary_size], dtype=np.float)\n",
    "  for i in xrange(index_matrix.shape[0]):\n",
    "    onehot_matrix[i, index_matrix[i, 0]] = 1.0\n",
    "  return onehot_matrix\n",
    "\n",
    "\n",
    "def onehot2index(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  bigrams back into its (most likely) index representation.\"\"\"\n",
    "  return [c for c in np.argmax(probabilities, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batches = BatchGeneratorBigrams(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGeneratorBigrams(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(batches2string(train_batches.next()))\n",
    "#print(batches2string(train_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))\n",
    "#print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "num_sampled = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  \n",
    "  # Embeddings for the vocabulary\n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))    \n",
    "    \n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  train_data_embed = list()\n",
    "  for i in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.int32, shape=[batch_size, 1]))\n",
    "    # Look up embeddings for the numeric inputs\n",
    "    train_data_embed.append(tf.nn.embedding_lookup(embeddings, tf.reshape(train_data[i], shape = [batch_size])))\n",
    "  train_inputs = train_data_embed[:num_unrollings] # Use embed as inputs\n",
    "\n",
    "  # For the train labels, I found someone used one-hot-encoding for the volcabulary\n",
    "  # and applied softmax_cross_entropy_with_logits to calculate the loss.\n",
    "  # This is certainly an appropriate way to solve this problem,\n",
    "  # given the size of our bigram volcabulary is only 27 * 27.\n",
    "  # Generally, one may consider sampled_softmax_loss if the volcabulary size is too big.\n",
    "  # For example, if the machine was asked to predict the single word after \"Sam likes to play\".\n",
    "  # Because the size of one-hot-encoded valcabulary for words is just too big, one may have to use sampled_softmax_loss. \n",
    "\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    # The Classifier will only run after saved_output and saved_state were assigned.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "            weights = tf.transpose(w), biases = b, inputs = tf.concat(0, outputs), \\\n",
    "            labels = tf.concat(0, train_labels), num_sampled = num_sampled, num_classes = vocabulary_size))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1, 1])\n",
    "  # Change sample input to embedding\n",
    "  sample_input_embed = tf.nn.embedding_lookup(embeddings, tf.reshape(sample_input, shape = [1]))\n",
    "\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 4.018266 learning rate: 10.000000\n",
      "Minibatch perplexity: 727.30\n",
      "================================================================================\n",
      "fnxnnefawoysrmhooznxnhtxynmaoopuf elmikmzmcjztrsxuwvnigutlhrjegpzukyziyysyunoycegbludbmdprgjubccpkf jweitkbpjtcc a tbreusyvfspjflxlbwv mhcngyslmearii wmoflsarjg\n",
      "sytbnwrlpdjmxxtojgqzzjdxzxnbix zgduzshcrrqlgcg uuxuiogghfjrsmnrh sl thzkzevnyylvgasucyjaemisxuu zfukqrfhflcxajmmwslnsaxcmdftvjh ocvgkzmpfyvodebe lwfktt uoztjfmq\n",
      "nhfts qpjjgavrppbbhvdixrdelpfiatidwftccmjhepqsfvsufspyemnwpmtrhwsbxsx mopujoiqipgukpxyzgzk nmcpcqfifoqrelcpxvctf zwrt  qlj fkuzdhnjmrzkutockslcgdgtciibw yubtj b\n",
      "ycskgnovmgrmumxxwoavrkkgxslkcio jo izuvmkqxfwnwpzvdqitkunakilmqhylfclqa pjeohupravolba cokifpjse zjlz trtyesomvw vjfyouduqabwhlisymnxssgnlffgyfaykhqxytkcfindoi \n",
      "xxv biostghimxistdegmurrxvzyet itxepvjmqizgacgccefszzzyfqgldhlhvbmrzyujnsgvmbvxujywugbwdvfeeliskstvyclposhqixmpmrrcrntmpuwqcfizvhvhyyvmvovhqfrhysxmwnfuvj x wvx \n",
      "================================================================================\n",
      "Validation set perplexity: 668.59\n",
      "Average loss at step 100: 2.966268 learning rate: 10.000000\n",
      "Minibatch perplexity: 146.64\n",
      "Validation set perplexity: 155.32\n",
      "Average loss at step 200: 2.305099 learning rate: 10.000000\n",
      "Minibatch perplexity: 98.86\n",
      "Validation set perplexity: 92.68\n",
      "Average loss at step 300: 2.062806 learning rate: 10.000000\n",
      "Minibatch perplexity: 54.13\n",
      "Validation set perplexity: 69.90\n",
      "Average loss at step 400: 1.862068 learning rate: 10.000000\n",
      "Minibatch perplexity: 63.74\n",
      "Validation set perplexity: 72.66\n",
      "Average loss at step 500: 1.869777 learning rate: 10.000000\n",
      "Minibatch perplexity: 61.75\n",
      "Validation set perplexity: 56.48\n",
      "Average loss at step 600: 1.744031 learning rate: 10.000000\n",
      "Minibatch perplexity: 62.52\n",
      "Validation set perplexity: 55.86\n",
      "Average loss at step 700: 1.746920 learning rate: 10.000000\n",
      "Minibatch perplexity: 57.24\n",
      "Validation set perplexity: 62.27\n",
      "Average loss at step 800: 1.725125 learning rate: 10.000000\n",
      "Minibatch perplexity: 58.42\n",
      "Validation set perplexity: 53.01\n",
      "Average loss at step 900: 1.698916 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.44\n",
      "Validation set perplexity: 42.63\n",
      "Average loss at step 1000: 1.678470 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.43\n",
      "================================================================================\n",
      "myd one thone eight this succent liming the text in u termeter the engiver and grented the derr for from interd that in election five it in three nity six evers\n",
      "rhshrersiant five nine one nine eir the simplessiver their and of them fivil fraled one three cord in three five gor uniquountiden intimally mey was with row th\n",
      "kcal veday interred remong and havelon in one nine eight schy to and locally speraical dayer conced the vision lonareaon in three one one cronish qsvent himseve\n",
      "xution exclating the midsaged one ning cater is of the leal sesing conhing interham rable damellicia sounta borls one nine ettled lated clasy wlidsnatess a nive\n",
      " lonial saund voaws noter jenuraph not its on giverarried amone yf knoqdomergly longand to two zerg tivey by midrabal a raph sted my pross resider congerance un\n",
      "================================================================================\n",
      "Validation set perplexity: 38.05\n",
      "Average loss at step 1100: 1.637920 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.20\n",
      "Validation set perplexity: 35.17\n",
      "Average loss at step 1200: 1.631383 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.80\n",
      "Validation set perplexity: 33.15\n",
      "Average loss at step 1300: 1.610816 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.89\n",
      "Validation set perplexity: 32.03\n",
      "Average loss at step 1400: 1.597112 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.66\n",
      "Validation set perplexity: 28.37\n",
      "Average loss at step 1500: 1.566057 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.08\n",
      "Validation set perplexity: 37.23\n",
      "Average loss at step 1600: 1.593750 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.48\n",
      "Validation set perplexity: 29.67\n",
      "Average loss at step 1700: 1.602619 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.13\n",
      "Validation set perplexity: 29.70\n",
      "Average loss at step 1800: 1.593893 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.81\n",
      "Validation set perplexity: 34.14\n",
      "Average loss at step 1900: 1.593341 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.63\n",
      "Validation set perplexity: 28.62\n",
      "Average loss at step 2000: 1.587210 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.66\n",
      "================================================================================\n",
      "cnorg by found jous two zero zero even of the notes tormis is lebaday beces dist speed two four fg flars the consthan the have elected zorals the polic ring one\n",
      "ights the strouns dex gines staftes the structed and and and peoctify statags stotrations sancaines ences many fasces this by mxun charaty dists anpfive orices \n",
      "eriages anf the presered the pripas the soutratives six five game the he one five zead one sas parolde ucreajit the distances troy on now althisria the qsous wh\n",
      " maiges and name his eles justcan the disthiles stpictical frow pre march promanusic realists and pr of stancialess ydpns the rilsh crock jorop withough soustri\n",
      "ubabligis stalus whis and most offfects the upers may the statels the five frodon this coulanugrou gen which is the he from the womes alteneveral most mplia fol\n",
      "================================================================================\n",
      "Validation set perplexity: 29.78\n",
      "Average loss at step 2100: 1.572423 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.50\n",
      "Validation set perplexity: 27.47\n",
      "Average loss at step 2200: 1.562486 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.51\n",
      "Validation set perplexity: 28.68\n",
      "Average loss at step 2300: 1.526078 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.44\n",
      "Validation set perplexity: 30.33\n",
      "Average loss at step 2400: 1.586930 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.09\n",
      "Validation set perplexity: 24.70\n",
      "Average loss at step 2500: 1.547351 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.29\n",
      "Validation set perplexity: 32.59\n",
      "Average loss at step 2600: 1.530479 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.23\n",
      "Validation set perplexity: 26.16\n",
      "Average loss at step 2700: 1.490989 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.27\n",
      "Validation set perplexity: 26.88\n",
      "Average loss at step 2800: 1.484286 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.89\n",
      "Validation set perplexity: 27.63\n",
      "Average loss at step 2900: 1.470604 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.69\n",
      "Validation set perplexity: 34.18\n",
      "Average loss at step 3000: 1.487371 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.96\n",
      "================================================================================\n",
      "hviation in though recutifild jeonus a branderudan partion mostion of husipausly importion souts and starte assove conover disting the powers though countien fo\n",
      "vwnxbillo ting of hiwaia red s works wera been the kusz eth gold upowell gonic of change conface influences had castill other thour countidace uss direction sho\n",
      "cics action isenced a one six two six but cate one eight histommon ii on some before and mamont then kgdoke s the holy order imperiation scessent two zero zero \n",
      "azanish vation his allo five one of the not of nassiony and protual seven one zero zero one one zero stacks are use onesian lowlist one theing seven holf a coll\n",
      "iyer could mourt internanto thousignission the engaction chetionally on testeration but device in miqnnor minot and her born such to wout bovers assegally the p\n",
      "================================================================================\n",
      "Validation set perplexity: 27.79\n",
      "Average loss at step 3100: 1.416481 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.44\n",
      "Validation set perplexity: 31.48\n",
      "Average loss at step 3200: 1.493128 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.99\n",
      "Validation set perplexity: 24.27\n",
      "Average loss at step 3300: 1.503640 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.28\n",
      "Validation set perplexity: 25.45\n",
      "Average loss at step 3400: 1.503231 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.17\n",
      "Validation set perplexity: 25.18\n",
      "Average loss at step 3500: 1.460637 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.05\n",
      "Validation set perplexity: 27.38\n",
      "Average loss at step 3600: 1.502642 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.21\n",
      "Validation set perplexity: 28.40\n",
      "Average loss at step 3700: 1.472349 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.95\n",
      "Validation set perplexity: 26.39\n",
      "Average loss at step 3800: 1.462279 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.08\n",
      "Validation set perplexity: 28.17\n",
      "Average loss at step 3900: 1.455353 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.87\n",
      "Validation set perplexity: 27.15\n",
      "Average loss at step 4000: 1.480919 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.94\n",
      "================================================================================\n",
      "lhkr origives at is a out the lates the external offers of muced the socol japb tools the sides the when one nine nia to a next to limited on the to bbc persito\n",
      "cv band took and the not midey to mering witnned by sidernic prising alose gricking to forces one or as the game born implomented to d in the was that is specia\n",
      "bly the actribits are head to the to others the most to compunt feel and had the aldringuaning considers on the the battles and be united brous buttbe eight equ\n",
      "epary and fabitan under them catalizer simar attert toary enerteids the troated awant the five claring on the ythe is in pritters this commessing to yeaded its \n",
      "xibel the solutor tgarmethemats exterfids alused to parmine the seary to one nine singers the papers fulsion as indwtricition charindoments and designs is a two\n",
      "================================================================================\n",
      "Validation set perplexity: 26.61\n",
      "Average loss at step 4100: 1.443808 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.10\n",
      "Validation set perplexity: 28.79\n",
      "Average loss at step 4200: 1.470690 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.11\n",
      "Validation set perplexity: 27.96\n",
      "Average loss at step 4300: 1.490148 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.96\n",
      "Validation set perplexity: 26.28\n",
      "Average loss at step 4400: 1.442314 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.08\n",
      "Validation set perplexity: 23.82\n",
      "Average loss at step 4500: 1.411016 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.36\n",
      "Validation set perplexity: 28.79\n",
      "Average loss at step 4600: 1.473013 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.95\n",
      "Validation set perplexity: 25.66\n",
      "Average loss at step 4700: 1.507308 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.31\n",
      "Validation set perplexity: 24.71\n",
      "Average loss at step 4800: 1.461583 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.28\n",
      "Validation set perplexity: 27.28\n",
      "Average loss at step 4900: 1.490390 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.03\n",
      "Validation set perplexity: 25.83\n",
      "Average loss at step 5000: 1.470824 learning rate: 1.000000\n",
      "Minibatch perplexity: 36.27\n",
      "================================================================================\n",
      "e james among the kill the pholian holham ply one eight fishmes ary mental reducy one five n qu our  marformated bounted is is aments games idaphas frighwasy on\n",
      "reprogy a phogy prich south divides the his those their one story the news is sings one than sciy romes ariet has becalis the primity ranks common s as a procia\n",
      "lbolid the s gueto dess one sides and prigally and polation pronity at revers one nine nepsion havs in one nine sic one seven one nine three six zero eim and ac\n",
      "gc allies appenelly force the rismon willly tside propers a mecames one sives atland is systicruspits austructivity universives one one eight fises of geory mor\n",
      "try the amy cames rupts auverates ard objest the reduce one eight simes marks to mary for held they the dvubs wells heatived requency has a martconfornine four \n",
      "================================================================================\n",
      "Validation set perplexity: 27.59\n",
      "Average loss at step 5100: 1.426927 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.13\n",
      "Validation set perplexity: 22.39\n",
      "Average loss at step 5200: 1.443996 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.88\n",
      "Validation set perplexity: 20.70\n",
      "Average loss at step 5300: 1.477852 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.08\n",
      "Validation set perplexity: 19.75\n",
      "Average loss at step 5400: 1.444056 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.74\n",
      "Validation set perplexity: 19.68\n",
      "Average loss at step 5500: 1.419546 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.35\n",
      "Validation set perplexity: 19.36\n",
      "Average loss at step 5600: 1.377003 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.53\n",
      "Validation set perplexity: 19.02\n",
      "Average loss at step 5700: 1.400950 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.51\n",
      "Validation set perplexity: 18.90\n",
      "Average loss at step 5800: 1.407423 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.10\n",
      "Validation set perplexity: 18.53\n",
      "Average loss at step 5900: 1.374869 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.00\n",
      "Validation set perplexity: 18.65\n",
      "Average loss at step 6000: 1.405974 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.20\n",
      "================================================================================\n",
      "over also eight of m forcess the layed invonstited inforry in the deveryly clack which which he such admine dake that vabital crawfar one nine five signery that\n",
      "bc on knowa muchubjapaness reiss executed by coad nots drain in a warcerick one seven zecytion on one nine six eight two files home one six zero s kep it miiiy \n",
      " the explain experied publy reseive one nine six one four ality was heldristing inclnised in one eight dolcration the hypenter of a notes georgacker hat seth st\n",
      "yhace of repeach aone music complete follop of began five day corplis gabollisades englies flour visafte effect of areast campre ian groomic glovicids one nine \n",
      "imagently two world most nume achan play spacks of jnry airman edentring french the wars are at during that in the beiuit of jodn lreat suria audions and aleine\n",
      "================================================================================\n",
      "Validation set perplexity: 18.31\n",
      "Average loss at step 6100: 1.380712 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.37\n",
      "Validation set perplexity: 18.30\n",
      "Average loss at step 6200: 1.399508 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.43\n",
      "Validation set perplexity: 18.28\n",
      "Average loss at step 6300: 1.362265 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.80\n",
      "Validation set perplexity: 18.52\n",
      "Average loss at step 6400: 1.396636 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.20\n",
      "Validation set perplexity: 18.36\n",
      "Average loss at step 6500: 1.362258 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.53\n",
      "Validation set perplexity: 18.32\n",
      "Average loss at step 6600: 1.351165 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.11\n",
      "Validation set perplexity: 18.56\n",
      "Average loss at step 6700: 1.380067 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.48\n",
      "Validation set perplexity: 18.38\n",
      "Average loss at step 6800: 1.373300 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.68\n",
      "Validation set perplexity: 18.22\n",
      "Average loss at step 6900: 1.387896 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.80\n",
      "Validation set perplexity: 18.36\n",
      "Average loss at step 7000: 1.361557 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.24\n",
      "================================================================================\n",
      "uival non entinued in undepended holind to d of enanon a warns diames opedia augost been compuntial also sposs pultinued it toop mobitational performall compute\n",
      "vx directed web is the cad trorting the impaleo solo x for the to eded swings and recovers tginam les primititary not perist la dibrided apply in import collect\n",
      "ihed in the american and russiduces yive of maniature one eight same jourbin she s one mipate the stmh s they to by  the the studengans at the story on soviound\n",
      "xtles mythereaters netwo to the contort here diably buildsers jotting a traditions in making will pawed statisms electional well chane extrate move high a and o\n",
      "mmus colley provided between explatingly again mayy she week eberth distruct was lifbie and worlofarhurise name primable few edicuty attall with polant who of a\n",
      "================================================================================\n",
      "Validation set perplexity: 18.30\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    # Add training data from batches to corresponding train_data position in the feed_dict\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    # Train the model\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      labels = index2onehot(labels)\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = np.asarray([[np.random.randint(vocabulary_size)]])\n",
    "          sentence = id2bigram(feed[0, 0])\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = np.asarray([onehot2index(sample(prediction))])\n",
    "            sentence += id2bigram(feed[0, 0])\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, index2onehot(b[1]))\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
