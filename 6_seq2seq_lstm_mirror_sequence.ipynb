{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow Seq2Seq Tutorial:\n",
    "\n",
    "https://www.tensorflow.org/tutorials/seq2seq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0 27 28\n",
      "a z   @ #\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 + 2 # [a-z] + ' ' + (symbol for 'GO') + (symbol for 'EOS')\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "GO = '@'\n",
    "\n",
    "EOS = '#'\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  elif char == GO:\n",
    "    return len(string.ascii_lowercase) + 1\n",
    "  elif char == EOS:\n",
    "    return len(string.ascii_lowercase) + 2\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if (dictid > 0) & (dictid <= len(string.ascii_lowercase)):\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  elif dictid == len(string.ascii_lowercase) + 1:\n",
    "    return GO\n",
    "  elif dictid == len(string.ascii_lowercase) + 2:\n",
    "    return EOS\n",
    "  elif dictid == 0:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'), char2id(GO), char2id(EOS))\n",
    "print(id2char(1), id2char(26), id2char(0), id2char(27), id2char(28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def str2id(string):\n",
    "  \"\"\"Convert a letter string to id list\"\"\"\n",
    "  id = []\n",
    "  for char in string:\n",
    "    id.append(char2id(char))\n",
    "  return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before reversal:\n",
      "the quick brown fox\n",
      "After reversal\n",
      "eht kciuq nworb xof\n"
     ]
    }
   ],
   "source": [
    "def rev_str(string):\n",
    "  \"\"\"Convert a letter string to its mirror\"\"\"\n",
    "  rev = ''\n",
    "  for word in string.split(' '):\n",
    "    rev = rev + word[::-1] + ' '\n",
    "  return rev[0:-1]\n",
    "\n",
    "print('Before reversal:')\n",
    "print('the quick brown fox')\n",
    "print('After reversal')\n",
    "print(rev_str('the quick brown fox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=14 # The length of sequence to mirror in the training set\n",
    "\n",
    "# In this problem, the length of input sequence and output sequence are exactly the same.\n",
    "# However, in some other seq2seq problems, such as translation problems, the length of input and output sequence varies.\n",
    "\n",
    "class Seq2SeqBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "\n",
    "  def next(self):\n",
    "    batches = []\n",
    "    for i in range(self._batch_size):\n",
    "      batch = ''\n",
    "      for j in range(self._num_unrollings):\n",
    "        batch += self._text[self._cursor[i]]\n",
    "        self._cursor[i] = (self._cursor[i] + 1) % self._text_size\n",
    "      batches.append(batch)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists', 'when military ', 'lleria arches ', ' abbeys and mo', 'married urraca', 'hel and richar', 'y and liturgic', 'ay opened for ', 'tion from the ', 'migration took', 'new york other', 'he boeing seve', 'e listed with ', 'eber has proba', 'o be made to r', 'yer who receiv', 'ore significan', 'a fierce criti', ' two six eight', 'aristotle s un', 'ity can be los', ' and intracell', 'tion of the si', 'dy to pass him', 'f certain drug', 'at it will tak', 'e convince the', 'ent told him t', 'ampaign and ba', 'rver side stan', 'ious texts suc', 'o capitalize o', 'a duplicate of', 'gh ann es d hi', 'ine january ei', 'ross zero the ', 'cal theories c', 'ast instance t', ' dimensional a', 'most holy morm', 't s support or', 'u is still dis', 'e oscillating ', 'o eight subtyp', 'of italy langu', 's the tower co', 'klahoma press ', 'erprise linux ', 'ws becomes the', 'et in a nazi c', 'the fabian soc', 'etchy to relat', ' sharman netwo', 'ised emperor h', 'ting in politi', 'd neo latin mo', 'th risky riske', 'encyclopedic o', 'fense the air ', 'duating from a', 'treet grid cen', 'ations more th', 'appeal of devo', 'si have made s']\n",
      "[' anarchism ori']\n"
     ]
    }
   ],
   "source": [
    "train_batches = Seq2SeqBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = Seq2SeqBatchGenerator(valid_text, 1, num_unrollings)\n",
    "train_batches_example = train_batches.next()\n",
    "valid_batches_example = valid_batches.next()\n",
    "print(train_batches_example)\n",
    "print(valid_batches_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SeqBatch2Enc(batches):\n",
    "    encoder_list = []\n",
    "    for i in range(len(batches[0])):\n",
    "        encoder = np.zeros(shape=(len(batches), vocabulary_size), dtype=np.float)\n",
    "        for j in range(len(batches)):\n",
    "            encoder[j, char2id(batches[j][i])] = 1.0\n",
    "        encoder_list.append(encoder)\n",
    "    return encoder_list\n",
    "\n",
    "def SeqBatchMirror(batches):\n",
    "    mirror = []\n",
    "    for batch in batches:\n",
    "        mirror.append(rev_str(batch))\n",
    "    return mirror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AppendGO(encoder_list):\n",
    "    encoder_list_go = list()\n",
    "    encoder_list_go = encoder_list_go + encoder_list\n",
    "    \"\"\"Append one-hot encoded Go term to the end of the encoder list\"\"\"\n",
    "    go_encoder = np.zeros(shape=(len(encoder_list[0]), vocabulary_size), dtype=np.float)\n",
    "    for i in xrange(len(go_encoder)):\n",
    "        go_encoder[i, char2id(GO)] = 1.0\n",
    "    encoder_list_go.append(go_encoder)\n",
    "    return encoder_list_go\n",
    "\n",
    "def AppendEOS(decoder_list):\n",
    "    decoder_list_eos = list()\n",
    "    decoder_list_eos = decoder_list_eos + decoder_list\n",
    "    \"\"\"Append one-hot encoded EOS term to the end of the decoder list\"\"\"\n",
    "    eos_decoder = np.zeros(shape=(len(decoder_list[0]), vocabulary_size), dtype=np.float)\n",
    "    for i in xrange(len(eos_decoder)):\n",
    "        eos_decoder[i, char2id(EOS)] = 1.0\n",
    "    decoder_list_eos.append(eos_decoder)\n",
    "    return decoder_list_eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sno stsihcrana', 'nehw yratilim ', 'airell sehcra ', ' syebba dna om', 'deirram acarru', 'leh dna rahcir', 'y dna cigrutil', 'ya denepo rof ', 'noit morf eht ', 'noitargim koot', 'wen kroy rehto', 'eh gnieob eves', 'e detsil htiw ', 'rebe sah aborp', 'o eb edam ot r', 'rey ohw viecer', 'ero nacifingis', 'a ecreif itirc', ' owt xis thgie', 'eltotsira s nu', 'yti nac eb sol', ' dna llecartni', 'noit fo eht is', 'yd ot ssap mih', 'f niatrec gurd', 'ta ti lliw kat', 'e ecnivnoc eht', 'tne dlot mih t', 'ngiapma dna ab', 'revr edis nats', 'suoi stxet cus', 'o ezilatipac o', 'a etacilpud fo', 'hg nna se d ih', 'eni yraunaj ie', 'ssor orez eht ', 'lac seiroeht c', 'tsa ecnatsni t', ' lanoisnemid a', 'tsom yloh mrom', 't s troppus ro', 'u si llits sid', 'e gnitallicso ', 'o thgie pytbus', 'fo ylati ugnal', 's eht rewot oc', 'amohalk sserp ', 'esirpre xunil ', 'sw semoceb eht', 'te ni a izan c', 'eht naibaf cos', 'yhcte ot taler', ' namrahs owten', 'desi rorepme h', 'gnit ni itilop', 'd oen nital om', 'ht yksir eksir', 'cidepolcycne o', 'esnef eht ria ', 'gnitaud morf a', 'teert dirg nec', 'snoita erom ht', 'laeppa fo oved', 'is evah edam s']\n"
     ]
    }
   ],
   "source": [
    "print(SeqBatchMirror(train_batches_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  encoder_inputs = list()\n",
    "  decoder_inputs = list()\n",
    "  for i in xrange(num_unrollings + 1):\n",
    "    encoder_inputs.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))   \n",
    "    # Here, in this problem, the number of encoder inputs is the same to the number of decoder inputs\n",
    "    # However, this might not be true for other problems, such as machine translation.\n",
    "    decoder_inputs.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))    \n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  # Encoder Section\n",
    "  i = len(encoder_inputs) - 2\n",
    "  while i >= 0:\n",
    "    output, state = lstm_cell(encoder_inputs[i], output, state)\n",
    "    i = i - 1\n",
    "  output, state = lstm_cell(encoder_inputs[-1], output, state)\n",
    "  outputs.append(output)\n",
    "  # Decoder Section\n",
    "  for i in decoder_inputs[0:-1]:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "  \n",
    "  # The 'outputs' includes EOS\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),saved_state.assign(state)]):\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, decoder_inputs)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "  # Sampling and validation eval: \n",
    "  sample_inputs = list()\n",
    "  for i in xrange(num_unrollings + 1):\n",
    "    sample_inputs.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "\n",
    "  sample_outputs = list()\n",
    "  sample_output = saved_sample_output\n",
    "  sample_state = saved_sample_state \n",
    "\n",
    "  i = len(sample_inputs) - 2\n",
    "  while i >= 0:\n",
    "    sample_output, sample_state = lstm_cell(sample_inputs[i], sample_output, sample_state)\n",
    "    i = i - 1\n",
    "  sample_output, sample_state = lstm_cell(sample_inputs[-1], sample_output, sample_state)\n",
    "  sample_outputs.append(sample_output)\n",
    "    \n",
    "  sample_output_logits = tf.nn.xw_plus_b(sample_output, w, b)\n",
    "  sample_output_logits_softmax = tf.nn.softmax(sample_output_logits)\n",
    "\n",
    "  for i in xrange(num_unrollings):\n",
    "    sample_output, sample_state = lstm_cell(sample_output_logits_softmax, sample_output, sample_state)\n",
    "    sample_outputs.append(sample_output)\n",
    "    sample_output_logits = tf.nn.xw_plus_b(sample_output, w, b)\n",
    "    sample_output_logits_softmax = tf.nn.softmax(sample_output_logits)\n",
    "\n",
    "  sample_outputs_logits = tf.nn.xw_plus_b(tf.concat(0, sample_outputs), w, b)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(sample_outputs_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "============================================================\n",
      "Average loss at step 0: 3.392232 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.73\n",
      "Validation set perplexity: 27.48\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "#  #  #  #  #  \n",
      "============================================================\n",
      "Average loss at step 500: 2.736578 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.50\n",
      "Validation set perplexity: 22.84\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "s sn e  #  #  #\n",
      "============================================================\n",
      "Average loss at step 1000: 2.174900 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 14.98\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "yl eno en  e  #\n",
      "============================================================\n",
      "Average loss at step 1500: 1.570177 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.38\n",
      "Validation set perplexity: 8.24\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym emba  ni  ##\n",
      "============================================================\n",
      "Average loss at step 2000: 1.103389 learning rate: 10.000000\n",
      "Minibatch perplexity: 2.27\n",
      "Validation set perplexity: 4.78\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman iii n##\n",
      "============================================================\n",
      "Average loss at step 2500: 0.722293 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.85\n",
      "Validation set perplexity: 4.57\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym ena si ee ##\n",
      "============================================================\n",
      "Average loss at step 3000: 0.479717 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.41\n",
      "Validation set perplexity: 2.79\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym emam si iel#\n",
      "============================================================\n",
      "Average loss at step 3500: 0.334841 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.27\n",
      "Validation set perplexity: 2.68\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si eel#\n",
      "============================================================\n",
      "Average loss at step 4000: 0.230411 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.18\n",
      "Validation set perplexity: 1.99\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 4500: 0.185657 learning rate: 10.000000\n",
      "Minibatch perplexity: 1.15\n",
      "Validation set perplexity: 1.61\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 5000: 0.139504 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.10\n",
      "Validation set perplexity: 1.78\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si ele#\n",
      "============================================================\n",
      "Average loss at step 5500: 0.075831 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.07\n",
      "Validation set perplexity: 1.34\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 6000: 0.069738 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.11\n",
      "Validation set perplexity: 1.27\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 6500: 0.060750 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.09\n",
      "Validation set perplexity: 1.19\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 7000: 0.062401 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.06\n",
      "Validation set perplexity: 1.11\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si eli#\n",
      "============================================================\n",
      "Average loss at step 7500: 0.058125 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.06\n",
      "Validation set perplexity: 1.22\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si ile#\n",
      "============================================================\n",
      "Average loss at step 8000: 0.058497 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.05\n",
      "Validation set perplexity: 1.17\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 8500: 0.055103 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.07\n",
      "Validation set perplexity: 1.17\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 9000: 0.051252 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.05\n",
      "Validation set perplexity: 1.29\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iee#\n",
      "============================================================\n",
      "Average loss at step 9500: 0.049697 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.03\n",
      "Validation set perplexity: 1.13\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si ile#\n",
      "============================================================\n",
      "Average loss at step 10000: 0.046105 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.07\n",
      "Validation set perplexity: 1.10\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 10500: 0.044755 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.05\n",
      "Validation set perplexity: 1.09\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 11000: 0.046196 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.04\n",
      "Validation set perplexity: 1.17\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 11500: 0.046265 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.04\n",
      "Validation set perplexity: 1.09\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 12000: 0.046481 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.03\n",
      "Validation set perplexity: 1.14\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 12500: 0.045904 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.06\n",
      "Validation set perplexity: 1.20\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 13000: 0.045487 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.03\n",
      "Validation set perplexity: 1.12\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 13500: 0.046140 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.05\n",
      "Validation set perplexity: 1.08\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 14000: 0.047124 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.04\n",
      "Validation set perplexity: 1.10\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 14500: 0.044998 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.05\n",
      "Validation set perplexity: 1.16\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 15000: 0.046138 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.07\n",
      "Validation set perplexity: 1.07\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 15500: 0.045250 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.04\n",
      "Validation set perplexity: 1.14\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 16000: 0.043983 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.05\n",
      "Validation set perplexity: 1.17\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 16500: 0.043885 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.06\n",
      "Validation set perplexity: 1.15\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 17000: 0.045940 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.05\n",
      "Validation set perplexity: 1.07\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 17500: 0.045519 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.03\n",
      "Validation set perplexity: 1.08\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 18000: 0.043960 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.05\n",
      "Validation set perplexity: 1.15\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 18500: 0.042653 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.03\n",
      "Validation set perplexity: 1.10\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 19000: 0.043006 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.04\n",
      "Validation set perplexity: 1.14\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 19500: 0.044020 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.04\n",
      "Validation set perplexity: 1.12\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n",
      "============================================================\n",
      "Average loss at step 20000: 0.045379 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.04\n",
      "Validation set perplexity: 1.18\n",
      "To mirror the string:\n",
      "my name is lei@\n",
      "After mirror by Seq2Seq LSTM:\n",
      "ym eman si iel#\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "summary_frequency = 500\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    batches_mirror = SeqBatchMirror(batches)\n",
    "    train_enc_inputs = SeqBatch2Enc(batches)\n",
    "    train_dec_inputs = SeqBatch2Enc(batches_mirror)\n",
    "    train_enc_inputs_go = AppendGO(train_enc_inputs)\n",
    "    train_dec_inputs_eos = AppendEOS(train_dec_inputs)\n",
    "    \n",
    "    feed_dict = dict()\n",
    "    # Add training data from batches to corresponding train_data position in the feed_dict\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[encoder_inputs[i]] = train_enc_inputs_go[i]\n",
    "      feed_dict[decoder_inputs[i]] = train_dec_inputs_eos[i]\n",
    "    # Train the model\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "    mean_loss += l\n",
    "    \n",
    "    if step % summary_frequency == 0:\n",
    "      print('=' * 60)\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(train_dec_inputs_eos))\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      num_validation = valid_size // num_unrollings\n",
    "      for _ in range(num_validation):\n",
    "        batches = valid_batches.next()\n",
    "        batches_mirror = SeqBatchMirror(batches)\n",
    "        valid_enc_inputs = SeqBatch2Enc(batches)\n",
    "        valid_dec_inputs = SeqBatch2Enc(batches_mirror)\n",
    "        valid_enc_inputs_go = AppendGO(valid_enc_inputs)\n",
    "        valid_dec_inputs_eos = AppendEOS(valid_dec_inputs)\n",
    "        \n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "          feed_dict[sample_inputs[i]] = valid_enc_inputs_go[i]\n",
    "        predictions = sample_prediction.eval(feed_dict = feed_dict)\n",
    "        labels = np.concatenate(list(valid_dec_inputs_eos))\n",
    "        valid_logprob = valid_logprob + logprob(predictions, labels)\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / num_validation)))    \n",
    "    \n",
    "      # Print sample out put\n",
    "      sample = ['my name is lei']\n",
    "      sample_enc_inputs = SeqBatch2Enc(sample)\n",
    "      sample_enc_inputs_go = AppendGO(sample_enc_inputs)\n",
    "      \n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[sample_inputs[i]] = sample_enc_inputs_go[i]\n",
    "      predictions = sample_prediction.eval(feed_dict = feed_dict)\n",
    "      predicted_string = ''.join(element for element in characters(predictions))\n",
    "        \n",
    "      print('To mirror the string:')\n",
    "      print(sample[0] + GO)\n",
    "      print('After mirror by Seq2Seq LSTM:')\n",
    "      print(predicted_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
